{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import gammaln\n",
    "import random\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import itertools\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hLDA-nCRP Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRP(topic, gamma):\n",
    "\n",
    "    '''\n",
    "    CRP: Chinese restaurant process results in an array of the probability, which represents\n",
    "    the chance a new word gets assigned to different topics.\n",
    "    \n",
    "    Input:\n",
    "    topic: a list of lists. In each sublist of topic, it contains corresponding assigned words. \n",
    "    gamma: double, a parameter for CRP.\n",
    "    \n",
    "    Output: \n",
    "    crp_p: a 1 * T array, where T is the number of topics.\n",
    "    '''\n",
    "    \n",
    "    crp_p = np.zeros(len(topic)+1)\n",
    "    M = 0\n",
    "    for i in range(len(topic)):\n",
    "        M += len(topic[i])\n",
    "        \n",
    "    for i in range(len(topic)+1):\n",
    "        if i == 0:\n",
    "            crp_p[i] = gamma / (gamma + M) \n",
    "        else:\n",
    "            crp_p[i] = len(topic[i-1])/(gamma + M)\n",
    "    return crp_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_sampling(corpus, gamma):\n",
    "    \n",
    "    \"\"\"\n",
    "    topic_sampling: samples t topics and returns a list of lists. \n",
    "    \n",
    "    Input: \n",
    "    corpus: a list of lists, each sublist contains the word in the corresponding document\n",
    "    gamma: paramter\n",
    "    \n",
    "    Output:\n",
    "    topic: a list of lists, contains the assigned word in each topic\n",
    "    \"\"\"\n",
    "    \n",
    "    topic = []\n",
    "    flat_words = list(itertools.chain.from_iterable(corpus))\n",
    "    i = 0\n",
    "    while i < len(flat_words):\n",
    "        cm_prop = CRP(topic, gamma)/sum(CRP(topic, gamma))\n",
    "        theta = np.random.multinomial(1,cm_prop).argmax()\n",
    "        topic.append([flat_words[i]]) if theta == 0 else topic[theta-1].append(flat_words[i])\n",
    "        i+=1\n",
    "    return topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z(corpus, T, alpha, beta):\n",
    "    \n",
    "    \"\"\"\n",
    "    Z samples from LDA model\n",
    "    \n",
    "    Input: \n",
    "    corpus: a list of lists, each sublist contains the word in the corresponding document\n",
    "    topic: a list of lists, output of the topic_sampling\n",
    "    alpha, beta: parameter\n",
    "    \n",
    "    Output: \n",
    "    z_topic: a list of lists, drawn from topic\n",
    "    z_doc: a list of lists, record the document number of the word assigned to each topic\n",
    "    \"\"\"\n",
    "   \n",
    "    D = len(corpus)\n",
    "\n",
    "    num_vocab = 0\n",
    "    for i in range(D):\n",
    "        num_vocab += len(corpus[i])\n",
    "    z_topic=[[] for t in range(T)]\n",
    "    z_doc=[[] for t in range(T)]\n",
    "    dict = [[key,i] for i,c in enumerate(corpus) for j, key in enumerate(c)]  \n",
    "    \n",
    "    for e in dict:\n",
    "        wi,i,j,p = e[0],e[1],0,np.zeros(T) \n",
    "        while j < T:\n",
    "            lik=(z_topic[j].count(wi)+beta)/(len(z_topic[j]) +num_vocab*beta)\n",
    "            pri=(np.sum(np.isin(z_topic[j],corpus[i]))+alpha)/(len(corpus[i]) +T*alpha)\n",
    "            p[j]=lik * pri \n",
    "            j += 1\n",
    "        i_top = np.random.multinomial(1, p/np.sum(p)).argmax()\n",
    "        z_topic[i_top].append(wi)\n",
    "        z_doc[i_top].append(i)\n",
    "    \n",
    "    return list(filter(None, z_topic)), list(filter(None, z_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRP_prior(corpus, doc_topic, gamma):\n",
    "\n",
    "    \"\"\"\n",
    "    CRP_prior: construct the prior\n",
    "    \n",
    "    Input:\n",
    "    corpus: a list of lists, each sublist contains the word in the corresponding document\n",
    "    doc_label: a list of lists, record the document number of the word assigned to each topic\n",
    "    gamma: parameter\n",
    "    \n",
    "    Output: \n",
    "    doc_p: a D*T dimensional array, the probability of each topic for each document\n",
    "    \"\"\"\n",
    "    \n",
    "    doc_p = np.zeros((len(corpus), len(doc_topic)))\n",
    "    for i in range(len(corpus)):\n",
    "        doc = []\n",
    "        for j in range(len(doc_topic)):\n",
    "            doc_num = [num for num in doc_topic[j]]\n",
    "            doc.append(doc_num)\n",
    "        doc_p[i,:] = CRP(doc, gamma)[1:]\n",
    "    return doc_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_likelihood(corpus, topic, eta):\n",
    "    \n",
    "    \"\"\"\n",
    "    word_likelihood: calculate the likelihood given a particular choice of c\n",
    "    \n",
    "    Input: \n",
    "    corpus: a list of lists, each sublist contains the word in the corresponding document\n",
    "    topic: a list of lists, obtained from Z function\n",
    "    eta: parameter\n",
    "    \n",
    "    Output: \n",
    "    wm: a D*T array, where D is the number of documents and T is the number of topics\n",
    "    \"\"\"\n",
    "    \n",
    "    wm = np.zeros((len(corpus), len(topic)))\n",
    "    \n",
    "    W = 0\n",
    "    for i in range(len(corpus)):\n",
    "        W += len(corpus[i])\n",
    "    \n",
    "    for i in range(len(corpus)):\n",
    "        doc = corpus[i]\n",
    "        for j in range(len(topic)):\n",
    "            l = topic[j]\n",
    "            denom_1 = 1\n",
    "            num_2 = 1\n",
    "            \n",
    "            n_cml_m = len(l) - len([w for w in set(doc) if w in l])\n",
    "            num_1 = gammaln(n_cml_m + W * eta)\n",
    "            denom_2 = gammaln(len(l) + W * eta)\n",
    "            \n",
    "            for word in doc:\n",
    "                nw_cml_m = l.count(word) - doc.count(word)\n",
    "                if nw_cml_m <= 0:\n",
    "                    nw_cml_m = 0\n",
    "                \n",
    "                denom_1 += gammaln(nw_cml_m + eta)\n",
    "                num_2 += gammaln(l.count(word) + eta)\n",
    "            \n",
    "            wm[i,j] = num_1 + num_2 - denom_1 - denom_2\n",
    "        wm[i, :] = wm[i, :] + abs(min(wm[i, :]) + 0.1)\n",
    "    wm = wm/wm.sum(axis = 1)[:, np.newaxis]\n",
    "    return wm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_sampling(corpus, T , alpha, beta, gamma, eta, ite):\n",
    "    \n",
    "    \"\"\"\n",
    "    gibbs sampling: posterior probability based on the CRP prior and word likelihood\n",
    "    \n",
    "    Input:\n",
    "    corpus: a list of lists, each sublist contains the word in the corresponding document\n",
    "    T: a list of lists, obtained from Z function\n",
    "    alpha, beta, gamma, eta: parameter\n",
    "    ite: number of iterations\n",
    "    \n",
    "    Output:\n",
    "    w_topic: a list of lists, the distribution of words for topics\n",
    "    \"\"\"\n",
    "    \n",
    "    num_vocab = np.sum([len(x) for x in corpus])\n",
    "    gibbs = np.zeros((num_vocab, ite))\n",
    "    \n",
    "    \n",
    "    for it in range(ite):\n",
    "        doc_topic= Z(corpus, T, alpha, beta)[0]\n",
    "        doc_p = CRP_prior(corpus, doc_topic, gamma)\n",
    "        lik = word_likelihood(corpus, doc_topic, eta)\n",
    "        c_m = (lik * doc_p) / (lik * doc_p).sum(axis = 1).reshape(-1,1) #posterior\n",
    "        \n",
    "        g=[]\n",
    "        for i in range(len(corpus)):\n",
    "            if np.sum(c_m[i,:-1])>1:\n",
    "                c_m[i,:-1]=c_m[i,:-1]/np.sum(c_m[i,:-1])\n",
    "                c_m[i,-1]=0\n",
    "            for word in corpus[i]:\n",
    "                p = np.random.multinomial(1, c_m[i])\n",
    "                g.append(p.argmax())\n",
    "        \n",
    "        gibbs[:,it]=g\n",
    "    \n",
    "    t=[]\n",
    "    for i in range(num_vocab):\n",
    "        tt = int(Counter(gibbs[i,:]).most_common(1)[0][0])\n",
    "        t.append(tt)\n",
    "        \n",
    "    n_topic=np.max(t)+1\n",
    "\n",
    "    wn_topic = [[] for _ in range(n_topic)]\n",
    "    \n",
    "    n = 0\n",
    "    for doc in corpus:\n",
    "        wn_doc_topic = [[] for _ in range(n_topic)]\n",
    "        for word in doc:\n",
    "            k = t[n]\n",
    "            wn_doc_topic[k].append(word)\n",
    "            n += 1\n",
    "        for i in range(n_topic):\n",
    "            if len(wn_doc_topic[i]) != 0:\n",
    "                k = wn_doc_topic[i]\n",
    "                wn_topic[i].append(k)\n",
    "\n",
    "    wn_topic = [x for x in wn_topic if x != []]\n",
    "    return wn_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hLDA(corpus, alpha, beta, gamma, eta, ite, level,num=3):\n",
    "\n",
    "    \"\"\"\n",
    "    hLDA generates an n*1 list of lists, where n is the number of level\n",
    "    \n",
    "    Input:\n",
    "    corpus: a list of lists, contains words in each document\n",
    "    alpha, beta, gamma, eta: parameter\n",
    "    ite: number of iteration\n",
    "    level: number of level\n",
    "    \n",
    "    Output:\n",
    "    hLDA_tree: an n*1 list of lists\n",
    "    node: an n*1 list of lists\n",
    "    \"\"\"\n",
    "    \n",
    "    topic = topic_sampling(corpus, gamma)\n",
    "    topic = len(topic)\n",
    "    hLDA_tree = [[] for t in range(level)]\n",
    "    node_num = [[] for t in range(level+1)]\n",
    "    node_num[0].append(1)\n",
    "    \n",
    "    print(\"***LEVEL 0***\\n\")\n",
    " \n",
    "    # Initialize the tree:\n",
    "    wn_topic = gibbs_sampling(corpus, topic, alpha, beta, gamma, eta, ite)\n",
    "    node_topic = sum(wn_topic[0],[])\n",
    "    hLDA_tree[0].append(node_topic)\n",
    "    print_t = [i[0] for i in Counter(node_topic).most_common(num)]\n",
    "    print('NODE 1:',print_t)\n",
    "    tmp_tree = wn_topic[1:]\n",
    "    node_num[1].append(len(wn_topic[1:]))\n",
    "    \n",
    "    # Define helper function to expand the hLDA tree\n",
    "    def expand_hLDA_tree(tmp_tree, hLDA_tree, node_num, i, it):\n",
    "        j = 0\n",
    "        while j < it:\n",
    "            if len(tmp_tree)==0:\n",
    "                break\n",
    "            wn_topic1 = gibbs_sampling(tmp_tree[0], topic, alpha, beta, gamma, eta, ite)\n",
    "            node_topic1 = [n for w in wn_topic1[0] for n in w]\n",
    "            hLDA_tree[i].append(node_topic1)\n",
    "            tmp_tree.remove(tmp_tree[0])\n",
    "            print_t = [i[0] for i in Counter(node_topic1).most_common(num)]\n",
    "            print('NODE',j+1,\":\",print_t)\n",
    "            if wn_topic1[1:] != []: tmp_tree.extend(wn_topic1[1:]) \n",
    "            node_num[i+1].append(len(wn_topic1[1:]))\n",
    "            j+=1\n",
    "            \n",
    "    for i in range(1, level): \n",
    "        print(' ')\n",
    "        print(\"***LEVEL %d***\" % i)\n",
    "        it = sum(node_num[i])\n",
    "        expand_hLDA_tree(tmp_tree, hLDA_tree, node_num, i, it)\n",
    "    \n",
    "    return hLDA_tree, node_num[:level]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_plot(hLDA_object, num = 3, save = False):\n",
    "    \n",
    "    \"\"\"Plot topic tree.\"\"\"\n",
    "    \n",
    "    from IPython.display import Image, display\n",
    "    def viewPydot(pdot):\n",
    "        plt = Image(pdot.create_png())\n",
    "        display(plt)\n",
    "\n",
    "    words,struc = hLDA_object\n",
    "    graph = pydot.Dot(graph_type='graph')\n",
    "    end_index = [np.insert(np.cumsum(i),0,0) for i in struc]\n",
    "    \n",
    "    for level in range(len(struc)-1):\n",
    "        leaf_word = words[level + 1]\n",
    "        leaf_struc = struc[level + 1]\n",
    "        word = words[level]\n",
    "        end_leaf_index = end_index[level+1]\n",
    "\n",
    "        def node_plot(leaf_word, leaf_struc, end_leaf_index, word):\n",
    "            for i,e in enumerate(word):\n",
    "                root = '\\n'.join([x[0] for x in Counter(e).most_common(num)])\n",
    "                lf = leaf_word[end_leaf_index[i]:end_leaf_index[i+1]]  \n",
    "                for l in lf:\n",
    "                    leaf_w = '\\n'.join([x[0] for x in Counter(list(l)).most_common(num)])\n",
    "                    edge = pydot.Edge(root, leaf_w)\n",
    "                    graph.add_edge(edge)\n",
    "    \n",
    "        for w in word:\n",
    "            node_plot(leaf_word, leaf_struc, end_leaf_index, word)\n",
    "    \n",
    "    if save == True:\n",
    "        graph.write_png('graph.png')\n",
    "    \n",
    "    viewPydot(graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
