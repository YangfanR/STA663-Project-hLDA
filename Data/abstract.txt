Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.

We give a basic introduction to Gaussian Process regression models. We focus on understanding the role of the stochastic process and how it is used to define a distribution over functions. We present the simple equations for incorporating training data and examine how to learn the hyperparameters using the marginal likelihood. We explain the practical advantages of Gaussian Process and end with conclusions and a look at the current trends in GP work.

The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.

Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classifier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overfit rapidly.

TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor- Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous “parameter server” designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that Tensor- Flow achieves for several real-world applications.

This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classification tasks utilising models linear in the parameters. Although this framework is fully general, we illustrate our approach with a particular specialisation that we denote the 'relevance vector machine' (RVM), a model of identical functional form to the popular and state-of-the-art 'support vector machine' (SVM). We demonstrate that by exploiting a probabilistic Bayesian learning framework, we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while offering a number of additional advantages. These include the benefits of probabilistic predictions, automatic estimation of 'nuisance' parameters, and the facility to utilise arbitrary basis functions (e.g. non-'Mercer' kernels).

In this survey, we review work in machine learning on methods for handling data sets containing large amounts of irrelevant information. We focus on two key issues: the problem of selecting relevant features, and the problem of selecting relevant examples. We describe the advances that have been made on these topics in both empirical and theoretical work in machine learning, and we present a general framework that we use to compare different methods. We close with some challenges for future work in this area.

During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.

An overview of the Milky Way Galaxy is provided and aspects of astronomical background are considered, taking into account positions and coordinate systems, proper motions, parallax, radial velocities, stellar spectra, magnitudes and colors, absolute energy distributions, and astronomical catalogs and atlases. The physical properties of stars and the interstellar medium are examined, giving attention to stellar distances, stellar masses, stellar radii, an analysis of stellar spectra, spiral arm and disk stars, spheroidal-component stars, stellar structure and evolution, pulsating variable stars, and questions of interstellar absorption. The space distribution of stars and the chemical elements in the Milky Way Galaxy are discussed along with the large-scale structure and stellar content of galaxies, the solar motion, the stellar residual-velocity distribution, and the rotation of galaxies. A description is presented of the large-scale distribution of gas in galaxies, taking into consideration the distribution of neutral hydrogen in the Milky Way Galaxy and in other galaxies, molecular clouds in the Milky Way Galaxy, and the galactic center.

This chapter discusses the effects of atmospheric turbulence in optical astronomy, summarizes the present state of the theory, reviews the experimental checks that have been made, and discusses the implications in the domain of astronomical observations. Diffraction limited resolution has been obtained with large telescopes up to magnitude 13 by means of speckle interferometry. A Michelson interferometer is already working in the visible on a 20 m baseline up to magnitude 4, and many projects are nearing completion. However, such technological progress can be useful only if accurate quantitative measurements prove to be feasible through atmospheric turbulence. In addition, much work remains to be done before achieving a full understanding of atmospheric effects. The relationship between isoplanicity or speckle boiling and the structure of the atmosphere is still not clear.

Ground-based γ-ray astronomy, which provides access to the TeV energy range, is a young and rapidly developing discipline. Recent discoveries in this waveband have important consequences for a wide range of topics in astrophysics and astroparticle physics. This article is an attempt to review the experimental status of this field and to provide the basic formulae and concepts required to begin the interpretation of TeV observations.

Photons in the visible range form the basis of astronomy. They move in straight lines, which preserves source information, but they arise only very indirectly from nuclear or high-energy processes. Cosmic-ray particles, on the other hand, arise directly from high-energy processes in astronomical objects of various classes, but carry no information about source direction. Radio emissions are still more complex in origin. But γ-rays arise rather directly in nuclear or high-energy processes, and yet travel in straight lines. Processes which might give rise to continuous and discrete γ-ray spectra in astronomical objects are described, and possible source directions and intensities are estimated. Present limits were set by observations with little energy or angular discrimination; γ-ray studies made at balloon altitudes, with feasible discrimination, promise valuable information not otherwise attainable.

Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.

Learning to rank for Information Retrieval (IR) is a task to automatically construct a ranking model using training data, such that the model can sort new objects according to their degrees of relevance, preference, or importance. Many IR problems are by nature ranking problems, and many IR technologies can be potentially enhanced by using learning-to-rank techniques. The objective of this tutorial is to give an introduction to this research direction. Specifically, the existing learning-to-rank algorithms are reviewed and categorized into three approaches: the pointwise, pairwise, and listwise approaches. The advantages and disadvantages with each approach are analyzed, and the relationships between the loss functions used in these approaches and IR evaluation measures are discussed. Then the empirical evaluations on typical learning-to-rank methods are shown, with the LETOR collection as a benchmark dataset, which seems to suggest that the listwise approach be the most effective one among all the approaches. After that, a statistical ranking theory is introduced, which can describe different learning-to-rank algorithms, and be used to analyze their query-level generalization abilities. At the end of the tutorial, we provide a summary and discuss potential future work on learning to rank.

Currently, most approaches to retrieving textual materials from scientific databases depend on a lexical match between words in users’ requests and those in or assigned to documents in a database. Because of the tremendous diversity in the words people use to describe the same document, lexical methods are necessarily incomplete and imprecise. Using the singular value decomposition (SVD), one can take advantage of the implicit higher-order structure in the association of terms with documents by determining the SVD of large sparse term by document matrices. Terms and documents represented by 200–300 of the largest singular vectors are then matched against user queries. We call this retrieval method latent semantic indexing (LSI) because the subspace represents important associative relationships between terms and documents that are not evident in individual documents. LSI is a completely automatic yet intelligent indexing method, widely applicable, and a promising way to improve users’ access to many kinds of textual materials, or to documents and services for which textual descriptions are available. A survey of the computational requirements for managing LSI-encoded databases as well as current and future applications of LSI is presented.

The naive Bayes classifier, currently experiencing a renaissance ] in machine learning, has long been a core technique in information retrieval. We review some of the variations of naive Bayes models used for text retrieval and classification, focusing on the distributional assumptions made about word occurrences in documents.

In conventional information retrieval Boolean combinations of index terms are used to formulate the users' information requests. While any document is in principle retrievable by a Boolean query, the amount of output obtainable by Boolean processing is difficult to control, and the retrieved items are not ranked in any presumed order of importance to the user population. In the vector processing model of retrieval, the retrieved items are easily ranked in decreasing order of the query-record similarity, but the queries themselves are unstructured and expressed as simple sets of weighted index terms. A new, extended Boolean information retrieval system is introduced which is intermediate between the Boolean system of query processing and the vector processing model. The query structure inherent in the Boolean system is preserved, while at the same time weighted terms may be incorporated into both queries and stored documents; the retrieved output can also be ranked in strict similarity order with the user queries. A conventional retrieval system can be modified to make use of the extended system. Laboratory tests indicate that the extended system produces better retrieval output than either the Boolean or the vector processing systems.

This book deals with properties of vocabularies for indexing and searching document collections; the construction, organization, display, and maintenance of these vocabularies; and the vocabulary as a factor affecting the performance of retrieval systems. Most of the text is concerned with vocabularies for post-coordinate retrieval systems, with special emphasis on thesauri and machine-based systems. Vocabularies for pre-coordinate systems (e.g., alphabetical subject catalogs and classified catalogs) are discussed only briefly to provide historical perspective and for the light they shed on the problems of vocabulary control in general. A full range of vocabulary control possibilities is discussed, from highly structured thesauri and classification schemes to natural-language (free-text) searching. Also discussed are the following: characteristics and components of an index language, creation of index languages automatically, vocabulary in the on-line retrieval system, and some cost-effectiveness aspects of vocabulary control. The book is designed primarily for students of library and information science.

Extending beyond the boundaries of science, art, and culture, content-based multimedia information retrieval provides new paradigms and methods for searching through the myriad variety of media all over the world. This survey reviews 100+ recent articles on content-based multimedia information retrieval and discusses their role in current research directions which include browsing and search paradigms, user studies, affective computing, learning, semantic queries, new features and media types, high performance indexing, and evaluation techniques. Based on the current state of the art, we discuss the major challenges for the future.

We propose a new probabilistic approach to information retrieval based upon the ideas and methods of statistical machine translation. The central ingredient in this approach is a statistical model of how a user might distill or "translate" a given document into a query. To assess the relevance of a document to a user's query, we estimate the probability that the query would have been generated as a translation of the document, and factor in the user's general preferences in the form of a prior distribution over documents. We propose a simple, well motivated model of the document-to-query translation process, and describe an algorithm for learning the parameters of this model in an unsupervised manner from a collection of documents. As we show, one can view this approach as a generalization and justification of the "language modeling" strategy recently proposed by Ponte and Croft. In a series of experiments on TREC data, a simple translation-based retrieval system performs well in comparison to conventional retrieval techniques. This prototype system only begins to tap the full potential of translation-based retrieval.

An information retrieval system in accordance with the principles of the present invention maintains a database that defines a relational association between a plurality of informational items in the system. The relational association is based on historical navigational behavior of users of the information retrieval system, and includes a relationship type, which is based on the characteristic similarities between the informational items, and relationship strength, which is based on the historical frequency of any related informational items being selected by a user within the same information retrieval session. When a navigation from one informational item to another information item is detected, the relationship type and the relationship strength of the two informational items are determined and stored in the database. During a subsequent selection of an informational item, any related informational items related to the selected informational item are sorted based on the respective relationship types and relationship strengths, and are provided in a sorted list from which the user can select.

Thematic analysis is a poorly demarcated, rarely acknowledged, yet widely used qualitative analytic method within psychology. In this paper, we argue that it offers an accessible and theoretically flexible approach to analysing qualitative data. We outline what thematic analysis is, locating it in relation to other qualitative analytic methods that search for themes or patterns, and in relation to different epistemological and ontological positions. We then provide clear guidelines to those wanting to start thematic analysis, or conduct it in a more deliberate and rigorous way, and consider potential pitfalls in conducting thematic analysis. Finally, we outline the disadvantages and advantages of thematic analysis. We conclude by advocating thematic analysis as a useful and flexible method for qualitative research in and beyond psychology.

A systematic, authentic, and comprehensive survey of Adler's contributions to the theory and practice of psychology. The editors have tried to create "the equivalent of a textbook by Adler in Individual Psychology." Except for editorial comments interspersed among the chapters, and an 18-page introduction by the two editors, the complete text is in Adler's own words. The first part (8 chapters) deals with compensation; masculine protest; fictionalism and finalism; superiority strivings; social interest; degree of activity; and style of life. The second part (11 chapters) covers neurotic development and behavior; psychoses; psychosomatic disorders; therapy with children and adults; crime; general life problems; and problems of social psychology. Extensive general and Adler bibliographies.

Biomaterials have played an enormous role in the success of medical devices and drug delivery systems. We discuss here new challenges and directions in biomaterials research. These include synthetic replacements for biological tissues, designing materials for specific medical applications, and materials for new applications such as diagnostics and array technologies.

Hydrophilic polymers are the center of research emphasis in nanotechnology because of their perceived “intelligence”. They can be used as thin films, scaffolds, or nanoparticles in a wide range of biomedical and biological applications. Here we highlight recent developments in engineering uncrosslinked and crosslinked hydrophilic polymers for these applications. Natural, biohybrid, and synthetic hydrophilic polymers and hydrogels are analyzed and their thermodynamic responses are discussed. In addition, examples of the use of hydrogels for various therapeutic applications are given. We show how such systems' intelligent behavior can be used in sensors, microarrays, and imaging. Finally, we outline challenges for the future in integrating hydrogels into biomedical applications.

Gold colloids have fascinated scientists for over a century and are now heavily utilized in chemistry, biology, engineering, and medicine. Today these materials can be synthesized reproducibly, modified with seemingly limitless chemical functional groups, and, in certain cases, characterized with atomic‐level precision. This Review highlights recent advances in the synthesis, bioconjugation, and cellular uses of gold nanoconjugates. There are now many examples of highly sensitive and selective assays based upon gold nanoconjugates. In recent years, focus has turned to therapeutic possibilities for such materials. Structures which behave as gene‐regulating agents, drug carriers, imaging agents, and photoresponsive therapeutics have been developed and studied in the context of cells and many debilitating diseases. These structures are not simply chosen as alternatives to molecule‐based systems, but rather for their new physical and chemical properties, which confer substantive advantages in cellular and medical applications.

Terahertz irradiation and sensing is being applied for the first time to a wide range of fields outside the traditional niches of space science, molecular line spectroscopy, and plasma diagnostics. This paper surveys some of the terahertz measurements and applications of interest in the biological and medical sciences.

Despite over a century of applying organic synthesis to the search for drugs, we are still far from even a cursory examination of the vast number of possible small molecules that could be created. Indeed, a thorough examination of all ‘chemical space’ is practically impossible. Given this, what are the best strategies for identifying small molecules that modulate biological targets? And how might such strategies differ, depending on whether the primary goal is to understand biological systems or to develop potential drugs?

Chemiluminescence, the emission of light caused by a chemical reaction, is a phenomenon used for many applications and of wide biological importance. It occurs in bacteria and insects (including glow-worms and fireflies), in many of the animals in the deep sea and even in human cells. The last 25 years have witnessed fast progress in the elucidation of the reactions and mechanisms underlying bioluminescence and light production by synthetic systems. Together with the development of highly sensitive light detectors, this has made available new biomedical methods and has given rise to new concepts concerning the biology and pathology of the cell. The book describes the occurrence, chemistry and measurement of chemiluminescence. It deals with the biological function and evolutionary significance, and looks at the many biomedical applications. The author describes the uses of chemiluminescence to measure enzymes, substrates and metabolites, to detect the changes of calcium concentration in living cells, to determine oxygen radicals or to replace the radioactive labels in immunoassays. Future applications in research and clinical laboratories are also discussed.

The discovery of mouse embryonic stem (ES) cells >20 years ago represented a major advance in biology and experimental medicine, as it enabled the routine manipulation of the mouse genome. Along with the capacity to induce genetic modifications, ES cells provided the basis for establishing an in vitro model of early mammalian development and represented a putative new source of differentiated cell types for cell replacement therapy. While ES cells have been used extensively for creating mouse mutants for more than a decade, their application as a model for developmental biology has been limited and their use in cell replacement therapy remains a goal for many in the field. Recent advances in our understanding of ES cell differentiation, detailed in this review, have provided new insights essential for establishing ES cell-based developmental models and for the generation of clinically relevant populations for cell therapy.

The war against infectious bacteria is not over! Although vancomycin and glycopeptide antibiotics have provided a strong last line of defence against many drug‐resistant bacteria, their overuse has given rise to more dangerous strains of bacteria. An understanding of the chemistry and biology of these highly complex glycopeptides are destined to play a crucial role in the discovery of new antibiotics.

Coherent anti-Stokes Raman scattering (CARS) microscopy is a label-free imaging technique that is capable of real-time, nonperturbative examination of living cells and organisms based on molecular vibrational spectroscopy. Recent advances in detection schemes, understanding of contrast mechanisms, and developments of laser sources have enabled superb sensitivity and high time resolution. Emerging applications, such as metabolite and drug imaging and tumor identification, raise many exciting new possibilities for biology and medicine.

Microfabrication uses integrated-circuit manufacturing technology supplemented by its own processes to create objects with dimensions in the range of micrometers to millimeters. These objects can have miniature moving parts, stationary structures, or both. Microfabrication has been used for many applications in biology and medicine. These applications fall into four domains: tools for molecular biology and biochemistry, tools for cell biology, medical devices, and biosensors. Microfabricated device structures may provide significantly enhanced function with respect to a conventional device. Sometimes microfabrication can enable devices with novel capabilities. These enhancing and enabling qualities are conferred when microfabrication is used appropriately to address the right types of problems.

Carbon monoxide (CO), a product of organic oxidation processes, arises in vivo during cellular metabolism, most notably heme degradation. CO binds to the heme iron of most hemoproteins. Tissue hypoxia following hemoglobin saturation represents a principle cause of CO‐induced mortality in higher organisms, though cellular targets cannot be excluded. Despite extreme toxicity at high concentrations, low concentrations of CO can confer cytoprotection during ischemia/reperfusion or inflammation‐induced tissue injury. Likewise, heme oxygenase, an enzyme that produces CO, biliverdin and iron, as well as a secondary increase in ferritin synthesis, from the oxidation of heme, can confer protection in vivo and in vitro. CO has been shown to affect several intracellular signaling pathways, including guanylate cyclase, which generates guanosine 3′:5′ cyclic monophosphate and the mitogen‐activated protein kinases (MAPK). Such pathways mediate, in part, the known vasoregulatory, anti‐inflammatory, anti‐apoptotic and anti‐proliferative effects of this gas. Exogenous CO delivered at low concentrations is showing therapeutic potential as an anti‐inflammatory agent and as such can modulate numerous pathophysiological states. This review will delve into the biological significance and medical applications of this gas molecule.

In recent years, semiconducting polymer nanoparticles have attracted considerable attention because of their outstanding characteristics as fluorescent probes. These nanoparticles, which primarily consist of π‐conjugated polymers and are called polymer dots (Pdots) when they exhibit small particle size and high brightness, have demonstrated utility in a wide range of applications such as fluorescence imaging and biosensing. In this review, we summarize recent findings of the photophysical properties of Pdots which speak to the merits of these entities as fluorescent labels. This review also highlights the surface functionalization and biomolecular conjugation of Pdots, and their applications in cellular labeling, in vivo imaging, single‐particle tracking, biosensing, and drug delivery. We discuss the relationship between the physical properties and performance, and evaluate the merits and limitations of the Pdot probes for certain imaging tasks and fluorescence assays. We also tackle the current challenges of Pdots and share our perspective on the future directions of the field.

Various aspects of the participation of Fenton chemistry in biology and medicine are reviewed. Accumulated evidence shows that both hydroxyl radical and ferryl [Fe(IV)=O]2+ can be formed under a variety of Fenton and Fenton-like reactions. Some examples of metal-independent hydroxyl radical production are included. Extracellular Fenton reaction is illustrated by the white rot and brown rot wood-decaying fungi. The natural and practical utilization of catechol-driven Fenton reaction is also presented.

Our work has shown that certain ruthenium(II) arene complexes exhibit promising anticancer activity in vitro and in vivo. The complexes are stable and water-soluble, and their frameworks provide considerable scope for optimising the design, both in terms of their biological activity and for minimising side-effects by variations in the arene and the other coordinated ligands. Initial studies on amino acids and nucleotides suggest that kinetic and thermodynamic control over a wide spectrum of reactions of Ru(II) arene complexes with biomolecules can be achieved. These Ru(II) arene complexes appear to have an altered profile of biological activity in comparison with metal-based anticancer complexes currently in clinical use or on clinical trial.

Cell-penetrating peptides (CPPs) have found numerous applications in biology and medicine since the first synthetic cell-permeable sequence was identified two decades ago. Numerous types of drugs have been transported into cells using CPPs, including small-molecule pharmaceuticals, therapeutic proteins, and antisense oligonucleotides. Improved agents for medical imaging have been generated by conjugation with CPPs, with the appended peptides promoting cellular uptake and in some cases, cell-type specificity. Organelle-specific CPPs have also been generated, providing a means to target specific subcellular sites. This review highlights achievements in this area and illustrates the numerous examples where peptide chemistry was exploited as a means to provide new tools for biology and medicine.

A growing body of evidence indicate that carotenoids possess anticarcinogenic, anti-mutagenic and immunomodulating effects. Saffron obtained from the dried stigmas of Crocus sativus L., is an important spice, rich in carotenoids, consumed commonly in different parts of the world. Our laboratory first reported the anticancer activity of saffron extract (dimethyl-crocetin) against a wide spectrum of murine tumors and human leukemia cell lines. The present report reviews the role of saffron in serving as a chemopreventive agent in modifying cancer risk. Dose-dependent cytotoxic effect to carcinoma, sarcoma and leukemia cells in vitro were noted. Saffron delayed ascites tumor growth and increased the life span of the treated mice compared to untreated controls by 45-120%. In addition, it delayed the onset of papilloma growth, decreased incidence of squamous cell carcinoma and soft tissue sarcoma in treated mice. Understanding the mechanisms of action of saffron have been solitarily based on their carotenoid-like action. Our results indicated significant inhibition in the synthesis of nucleic acids but not protein synthesis. It appears now that saffron (dimethyl-crocetin) disrupts DNA-protein interactions e.g. topoisomerases II, important for cellular DNA synthesis.

We review P.A. Stewart's quantitative approach to acid-base chemistry, starting with its historical context. We outline its implications for cellular and membrane processes in acid-base physiology; discuss its contributions to the understanding and analysis of acid-base phenomena; show how it can be applied in clinical problems; and propose a classification of clinical acid-base disturbances based on this general approach.

The chemistry of graphene oxide is discussed in this critical review. Particular emphasis is directed toward the synthesis of graphene oxide, as well as its structure. Graphene oxide as a substrate for a variety of chemical transformations, including its reduction to graphene-like materials, is also discussed. This review will be of value to synthetic chemists interested in this emerging field of materials science, as well as those investigating applications of graphene who would find a more thorough treatment of the chemistry of graphene oxide useful in understanding the scope and limitations of current approaches which utilize this material. 

The author's objective was to write a text/reference book useful to established researchers and to students interested in the chemistry of the lower atmosphere and the experimental techniques used to study it. The book contains discussions of the fundamentals of kinetics, spectroscopy, and photochemistry that should prove helpful to students; descriptions of many experimental techniques used to study the atmosphere; and a great many rate constants, reaction mechanisms, and other relevant data for organic and inorganic species found in the atmosphere. The authors state in the preface that their efforts do not constitute a comprehensive survey of the literature; still, the volume contains extensive literature references ranging from the earliest in a specific area of mid-1985. All in all, the authors have achieved their objective very well.

It has become evident that fluorinated compounds have a remarkable record in medicinal chemistry and will play a continuing role in providing lead compounds for therapeutic applications. This tutorial review provides a sampling of renowned fluorinated drugs and their mode of action with a discussion clarifying the role and impact of fluorine substitution on drug potency.

In nucleotide excision repair DNA damage is removed through incision of the damaged strand on both sides of the lesion, followed by repair synthesis, which fills the gap using the intact strand as a template, and finally ligation. In prokaryotes the damaged base is removed in a 12-13 nucleotide (nt)-long oligomer; in eukaryotes including humans the damage is excised in a 24-32 nt-long fragment. Excision in Escherichia coli is accomplished by three proteins designated UvrA, UvrB, and UvrC. In humans, by contrast, 16 polypeptides including seven xeroderma pigmentosum (XP) proteins, the trimric replication protein A [RPA, human single-stranded DNA binding protein (HSSB)], and the multisubunit (7-10) general transcription factor TFIIH are required for the dual incisions. Transcribed strands are specifically targeted for excision repair by a transcription-repair coupling factor both in E. coli and in humans. In humans, excision repair is an important defense mechanism against the two major carcinogens, sunlight and cigarette smoke. Individuals defective in excision repair exhibit a high incidence of cancer while individuals with a defect in coupling transcription to repair suffer from neurological and skeletal abnormalities.

DNA repair is an important molecular defense system against agents that cause cancer, degenerative diseases, and aging. Several repair systems in humans protect the genome by repairing modified bases, DNA adducts, cross links, and double strand breaks. These repair systems, base excision, nucleotide excision, and recombination, are intimately connected to transcription and to cell cycle checkpoints. In addition, genotoxic stress induces a set of cellular reactions mediated by the p53 tumor suppressor and the Ras oncogene. These genotoxic response reactions may help the cell survive or enter apoptosis. Damage-response reactions may be utilized as targets of anticancer chemotherapy.

Xeroderma pigmentosum is a hereditary disease caused by defective DNA repair. Somatic cell genetics and biochemical studies with cell-free extracts indicate that at least 16 polypeptides are required to carry out the repair reaction proper, i.e. the removal of the lesion from the DNA by the dual incisions of the damaged strand. To find out if these proteins are necessary and sufficient for excision repair, they were obtained at a high level of purity in five fractions. The mixture of these five fractions reconstituted the excision nuclease (excinuclease) activity. Using the reconstituted excinuclease, we found that the excised fragment remains associated with the post-incision DNA-protein complex, suggesting that accessory proteins are needed to release the excised oligomer.

Photolyase uses light energy to split UV-induced cyclobutane dimers in damaged DNA, but its molecular mechanism has never been directly revealed. Here, we report the direct mapping of catalytic processes through femtosecond synchronization of the enzymatic dynamics with the repair function. We observed direct electron transfer from the excited flavin cofactor to the dimer in 170 ps and back electron transfer from the repaired thymines in 560 ps. Both reactions are strongly modulated by active-site solvation to achieve maximum repair efficiency. These results show that the photocycle of DNA repair by photolyase is through a radical mechanism and completed on subnanosecond time scale at the dynamic active site, with no net change in the redox state of the flavin cofactor.

A 32-base-pair DNA fragment containing a thymine photodimer was constructed and ligated head-to-tail to obtain multimers of this sequence in which thymine dimers were in phase with the helix screw axis (approximately equal to 3 turns apart). The ligation products were analyzed by one- and two-dimensional gel electrophoresis and quantitative electron microscopy. These analyses show that the thymine photodimer introduces a bend of approximately equal to 30 degrees in DNA, which causes anomalously slow migration of DNA fragments in polyacrylamide gels and facilitates the formation of small covalent circles. Repair of thymine dimers by DNA photolyase abolishes the anomalous migration.

A number of chemotherapeutic agents, such as platinum drugs, nitrogen mustards, and chloroethylnitrosoureas, act by forming bifunctional DNA adducts. It is likely that abortive attempts to replicate and/or repair the damaged DNA cause chromosome aberrations and breakage, leading to cell death. Any substantial increase in cellular capacity to repair damaged DNA may result in resistance to chemotherapeutic agents. In this review, we examine the types of DNA adducts formed by the major classes of chemotherapeutic agents, the enzymatic pathways that play a role in the repair of those adducts, the evidence that DNA repair is enhanced in drug-resistant cell lines and tumors, and strategies for utilizing selective inhibition of DNA repair to overcome resistance.

Escherichia coli DNA photolyase (photoreactivating enzyme) was purified to homogeneity from a strain that greatly overproduces the protein. The purified enzyme has absorbtion peaks at 280 and 380 nm, a fluorescence emission peak at 480 nm and, upon denaturation, releases a chromophore that has the spectroscopic properties of flavin adenine dinucleotide (FAD), indicating that FAD is an intrinsic chromophore of the enzyme.

Photolyases and cryptochrome blue-light photoreceptors are evolutionarily related flavoproteins that perform distinct functions. Photolyases repair UV-damaged DNA in many species from bacteria to plants and animals. Cryptochromes regulate growth and development in plants and the circadian clock in animals. Recently, a new branch of the photolyase/cryptochrome family was identified. Members of this branch exhibited no or trace levels of DNA repair activity in vivo and in vitro and, therefore, were considered to be cryptochromes, and they were named cryptochrome-DASH. Here, we show that Cry-DASH proteins from bacterial, plant, and animal sources actually are photolyases with high degree of specificity for cyclobutane pyrimidine dimers in ssDNA.

Caffeine potentiates the mutagenic and lethal effects of genotoxic agents. It is thought that this is due, at least in some organisms, to inhibition of DNA repair. However, direct evidence for inhibition of repair enzymes has been lacking. Using purified Escherichia coli DNA photolyase and (A)BC excinuclease, we show that the drug inhibits photoreactivation and nucleotide excision repair by two different mechanisms. Caffeine inhibits photoreactivation by interfering with the specific binding of photolyase to damaged DNA, and it inhibits nucleotide excision repair by promoting nonspecific binding of the damage-recognition subunit, UvrA, of (A)BC excinuclease. A number of other intercalators, including acriflavin and ethidium bromide, appear to inhibit the excinuclease by a similar mechanism--that is, by trapping the UvrA subunit in nonproductive complexes on undamaged DNA.

The most frequent DNA adduct made by the anticancer drug cisplatin, the 1,2-intrastrand d(GpG) cross-link, as well as the minor 1,3-intrastrand d(GpTpG) adduct, were both repaired by an in vitro human excision repair system. Fragments of 27-29 nt containing the platinum damage were excised. The high mobility group (HMG)-domain proteins HMG1 and human mitochondrial transcription factor specifically inhibited repair of the 1,2-intrastrand cross-link by the human excision nuclease. These results suggest that the types and levels of HMG-domain proteins in a given tumor may influence the responsiveness of that cancer to cisplatin chemotherapy and they provide a rational basis for the synthesis of new platinum anticancer drug candidates.

Bacterial and mammalian mismatch repair systems have been implicated in the cellular response to certain types of DNA damage, and genetic defects in this pathway are known to confer resistance to the cytotoxic effects of DNA-methylating agents. Such observations suggest that in addition to their ability to recognize DNA base-pairing errors, members of the MutS family may also respond to genetic lesions produced by DNA damage. We show that the human mismatch recognition activity MutSalpha recognizes several types of DNA lesion including the 1,2-intrastrand d(GpG) crosslink produced by cis-diamminedichloroplatinum(II), as well as base pairs between O6-methylguanine and thymine or cytosine, or between O4-methylthymine and adenine. However, the protein fails to recognize 1,3-intrastrand adduct produced by trans-diamminedichloroplatinum(II) at a d(GpTpG) sequence. These observations imply direct involvement of the mismatch repair system in the cytotoxic effects of DNA-methylating agents and suggest that recognition of 1,2-intrastrand cis-diamminedichloroplatinum(II) adducts by MutSalpha may be involved in the cytotoxic action of this chemotherapeutic agent.

By using a human cell-free system capable of nucleotide excision repair, a synthetic substrate consisting of a plasmid containing four thymidine dimers at unique locations, and deoxyribonucleoside 5'-[alpha-thio]triphosphates for repair synthesis, we obtained DNA fragments containing repair patches with phosphorothioate linkages. Based on the resistance of these linkages to digestion by exonuclease III and their sensitivity to cleavage by I2, we were able to delineate the borders of the repair patch to single-nucleotide resolution and found an asymmetric patch with sharp boundaries. That the repair patch was produced by filling in a gap generated by an excision nuclease and not by nick-translation was confirmed by the finding that the thymidine dimer was released in a 27- to 29-nucleotide oligomer.

Xeroderma pigmentosum (XP) patients fail to remove pyrimidine dimers caused by sunlight and, as a consequence, develop multiple cancers in areas exposed to light. The second most common sign, present in 20–30% of XP patients, is a set of neurological abnormalities caused by neuronal death in the central and peripheral nervous systems. Neural tissue is shielded from sunlight-induced DNA damage, so the cause of neurodegeneration in XP patients remains unexplained. In this study, we show that two major oxidative DNA lesions, 8-oxoguanine and thymine glycol, are excised from DNA in vitro by the same enzyme system responsible for removing pyrimidine dimers and other bulky DNA adducts. Our results suggest that XP neurological disease may be caused by defective repair of lesions that are produced in nerve cells by reactive oxygen species generated as by-products of an active oxidative metabolism.

Pyrimidine dimers are the major photoproducts produced in cellular DNA upon UV irradiation. In Escherichia coli there are dark and photorepair mechanisms that eliminate the dimers from DNA and prevent their lethal and mutagenic effects. To determine whether these repair mechanisms act cooperatively or competitively in repairing DNA, we investigated the effects upon one another of DNA photolyase, which mediates photorepair, and uvrABC excision nuclease, an enzyme complex of the uvrABC gene products, which catalyzes nucleotide excision repair. We found that photolyase stimulates the removal of pyrimidine dimers but not other DNA adducts by uvrABC excision nuclease. The two subunits of uvrABC excision nuclease, the uvrA and uvrB proteins which together bind to the dimer region of DNA, had no effect on the activity of photolyase. T4 endonuclease V, which like photolyase is specific for pyrimidine dimers, was inhibited by photolyase, suggesting that these two proteins recognize the same or similar chemical structures in UV-irradiated DNA that are different from those recognized by uvrABC excision nuclease.

Checkpoint Rad proteins function early in the DNA damage checkpoint signaling cascade to arrest cell cycle progression in response to DNA damage. This checkpoint ensures the transmission of an intact genetic complement to daughter cells. To learn about the damage sensor function of the human checkpoint Rad proteins, we purified a heteropentameric complex composed of hRad17-RFCp36-RFCp37-RFCp38-RFCp40 (hRad17-RFC) and a heterotrimeric complex composed of hRad9-hHus1-hRad1 (checkpoint 9-1-1 complex). hRad17-RFC binds to DNA, with a preference for primed DNA and possesses weak ATPase activity that is stimulated by primed DNA and single-stranded DNA. hRad17-RFC forms a complex with the 9-1-1 heterotrimer reminiscent of the replication factor C/proliferating cell nuclear antigen clamp loader/sliding clamp complex of the replication machinery. These findings constitute biochemical support for models regarding the roles of checkpoint Rads as damage sensors in the DNA damage checkpoint response of human cells.

Photoreactivating enzyme (DNA photolyase; deoxyribocyclobutadipyrimidine pyrimidine-lyase, EC 4.1.99.3) repairs UV damage to DNA by utilizing the energy of near-UV/visible light to split pyrimidine dimers into monomers. The enzyme is widespread in nature but is absent in certain species in a seemingly unpredictable manner. Its presence in humans has been a source of considerable controversy. To help resolve the issue we used a very specific and sensitive assay to compare photoreactivation activity in human, rattlesnake, yeast, and Escherichia coli cells. Photolyase was easily detectable in E. coli, yeast, and rattlesnake cell-free extracts but none was detected in cell-free extracts from HeLa cells or human white blood cells with an assay capable of detecting 10 molecules per cell. We conclude that humans most likely do not have DNA photolyase.

Nucleotide excision repair is a general repair system that eliminates many dissimilar lesions from DNA. In an effort to understand substrate determinants of this repair system, we tested DNAs with minor backbone modifications using the ultrasensitive excision assay. We found that a phosphorothioate and a methylphosphonate were excised with low efficiency. Surprisingly, we also found that fragments of 23–28 nucleotides and of 12–13 nucleotides characteristic of human and Escherichia coliexcision repair, respectively, were removed from undamaged DNA at a significant rate. Considering the relative abundance of undamaged DNA in comparison to damaged DNA in the course of the life of an organism, we conclude that, in general, excision from and resynthesis of undamaged DNA may exceed the excision and resynthesis caused by DNA damage. As resynthesis is invariably associated with mutations, we propose that gratuitous repair may be an important source of spontaneous mutations.

The human DNA damage sensors, Rad17-replication factor C (Rad17-RFC) and the Rad9-Rad1-Hus1 (9-1-1) checkpoint complex, are thought to be involved in the early steps of the DNA damage checkpoint response. Rad17-RFC and the 9-1-1 complex have been shown to be structurally similar to the replication factors, RFC clamp loader and proliferating cell nuclear antigen polymerase clamp, respectively. Here, we demonstrate functional similarities between the replication and checkpoint clamp loader/DNA clamp pairs. When all eight subunits of the two checkpoint complexes are coexpressed in insect cells, a stable Rad17-RFC/9-1-1 checkpoint supercomplex forms in vivo and is readily purified. The two individually purified checkpoint complexes also form a supercomplex in vitro, which depends on ATP and is mediated by interactions between Rad17 and Rad9. Rad17-RFC binds to nicked circular, gapped, and primed DNA and recruits the 9-1-1 complex in an ATP-dependent manner. Electron microscopic analyses of the reaction products indicate that the 9-1-1 ring is clamped around the DNA.

Mammalian cells possess a cell‐autonomous molecular clock which controls the timing of many biochemical reactions and hence the cellular response to environmental stimuli including genotoxic stress. The clock consists of an autoregulatory transcription–translation feedback loop made up of four genes/proteins, BMal1, Clock, Cryptochrome, and Period. The circadian clock has an intrinsic period of about 24 h, and it dictates the rates of many biochemical reactions as a function of the time of the day. Recently, it has become apparent that the circadian clock plays an important role in determining the strengths of cellular responses to DNA damage including repair, checkpoints, and apoptosis. These new insights are expected to guide development of novel mechanism‐based chemotherapeutic regimens.

DNA interstrand cross-links are induced by many carcinogens and anticancer drugs. It was previously shown that mammalian DNA excision repair nuclease makes dual incisions 5′ to the cross-linked base of a psoralen cross-link, generating a gap of 22 to 28 nucleotides adjacent to the cross-link. We wished to find the fates of the gap and the cross-link in this complex structure under conditions conducive to repair synthesis, using cell extracts from wild-type and cross-linker-sensitive mutant cell lines. We found that the extracts from both types of strains filled in the gap but were severely defective in ligating the resulting nick and incapable of removing the cross-link. The net result was a futile damage-induced DNA synthesis which converted a gap into a nick without removing the damage. In addition, in this study, we showed that the structure-specific endonuclease, the XPF-ERCC1 heterodimer, acted as a 3′-to-5′ exonuclease on cross-linked DNA in the presence of RPA. Collectively, these observations shed some light on the cellular processing of DNA cross-links and reveal that cross-links induce a futile DNA synthesis cycle that may constitute a signal for specific cellular responses to cross-linked DNA.

Photolyase uses blue light to restore the major ultraviolet (UV)-induced DNA damage, the cyclobutane pyrimidine dimer (CPD), to two normal bases by splitting the cyclobutane ring. Our earlier studies showed that the overall repair is completed in 700 ps through a cyclic electron-transfer radical mechanism. However, the two fundamental processes, electron-tunneling pathways and cyclobutane ring splitting, were not resolved. Here, we use ultrafast UV absorption spectroscopy to show that the CPD splits in two sequential steps within 90 ps and the electron tunnels between the cofactor and substrate through a remarkable route with an intervening adenine. Site-directed mutagenesis reveals that the active-site residues are critical to achieving high repair efficiency, a unique electrostatic environment to optimize the redox potentials and local flexibility, and thus balance all catalytic reactions to maximize enzyme activity. These key findings reveal the complete spatio-temporal molecular picture of CPD repair by photolyase and elucidate the underlying molecular mechanism of the enzyme’s high repair efficiency.

How much and how fast should we react to the threat of global warming? The Stern Review argues that the damages from climate change are large, and that nations should undertake sharp and immediate reductions in greenhouse gas emissions. An examination of the Review's radical revision of the economics of climate change finds, however, that it depends decisively on the assumption of a near-zero time discount rate combined with a specific utility function. The Review's unambiguous conclusions about the need for extreme immediate action will not survive the substitution of assumptions that are consistent with today's marketplace real interest rates and savings rates.

Large, abrupt, and widespread climate changes with major impacts have occurred repeatedly in the past, when the Earth system was forced across thresholds. Although abrupt climate changes can occur for many reasons, it is conceivable that human forcing of climate change is increasing the probability of large, abrupt events. Were such an event to recur, the economic and ecological impacts could be large and potentially serious. Unpredictability exhibited near climate thresholds in simple models shows that some uncertainty will always be associated with projections. In light of these uncertainties, policy-makers should consider expanding research into abrupt climate change, improving monitoring systems, and taking actions designed to enhance the adaptability and resilience of ecosystems and economies.

Designing efficient policies to slow global warming requires an approach that combines economic tools with relations from the natural sciences. The dynamic integrated climate-economy (DICE) model presented here, an intertemporal general-equilibrium model of economic growth and climate change, can be used to investigate alternative approaches to slowing climate change. Evaluation of five policies suggests that a modest carbon tax would be an efficient approach to slow global warming, whereas rigid emissions- or climate-stabilization approaches would impose significant net economic costs.

The linkage between economic activity and geography is obvious: Populations cluster mainly on coasts and rarely on ice sheets. Past studies of the relationships between economic activity and geography have been hampered by limited spatial data on economic activity. The present study introduces data on global economic activity, the G-Econ database, which measures economic activity for all large countries, measured at a 1° latitude by 1° longitude scale. The methodologies for the study are described. Three applications of the data are investigated. First, the puzzling “climate-output reversal” is detected, whereby the relationship between temperature and output is negative when measured on a per capita basis and strongly positive on a per area basis. Second, the database allows better resolution of the impact of geographic attributes on African poverty, finding geography is an important source of income differences relative to high-income regions. Finally, we use the G-Econ data to provide estimates of the economic impact of greenhouse warming, with larger estimates of warming damages than past studies.

Studies of environmental and climate-change policy-indeed of virtually all aspects of economic policy-have generally sidestepped the thorny issue of induced innovation, which refers to the impact of economic activity and policy on research, development, and the diffusion of new technologies. This omission arises both because of the lack of a firm empirical understanding of the determinants of technological change and because of the inherent difficulties of economic modeling processes with externalities and increasing returns to scale. While we suspect that we know the direction of this effect-toward overestimates of the cost of emissions reductions and the trend increase in climate change-we have little sense of the magnitude of the effect or the importance of this omission. Would including induced innovation have a large or small impact on climate change and on climate-change policies? This is a major open question.

Economic analyses of efficient policies to slow climate change require combining economic and scientific approaches. The present study presents a dynamic integrated climate-economy (‘DICE’) model. This model can be used to investigate alternative approaches to slowing climate change. Evaluation of five policies suggest that a modest carbon tax would be an efficient approach to slow global warming, while rigid emissions-stabilization approaches would impose significant net economic costs.

Nations generally measure their economic performance using the yardstick of national output and income. It is not widely recognized, however, that conventional measures of national income and output exclude the value of improvements in the health status of the population. The present study develops a methodology and presents preliminary estimates of how standard economic measures would change if they adequately reflected improvements in health status. The study first discusses the theory of the measurement of national income, examines some of the shortcomings of traditional concepts, and proposes a new concept called 'health income' that can be used to incorporate improvements in health status. The study next discusses how the proposed measure fits into existing theories for measuring and valuing consumption and health status. The study applies the new concepts to data for the United States over the twentieth century and concludes that accounting for improvements in the health status would substantially increase the estimated improvement in economic welfare for the U.S. over the twentieth century.

This study reviews different approaches to the political and economic control of global public goods such as global warming. It compares quantity-oriented control mechanisms like the Kyoto Protocol with price-type control mechanisms such as internationally harmonized carbon taxes. The analysis focuses on such issues as the relationship to ultimate targets, performance under conditions of uncertainty, volatility of induced carbon prices, the inefficiencies of taxation and regulation, potential for corruption and accounting finagling, and ease of implementation. It concludes that price-type approaches such as carbon taxes have major advantages for slowing global warming.

Growth in this model is driven by technological change that arises from intentional investment decisions made by profit-maximizing agents. The distinguishing feature of the technology as an input is that it is neither a conventional good nor a public good; it is a nonrival, partially excludable good. Because of the nonconvexity introduced by a nonrival good, price-taking competition cannot be supported. Instead, the equilibrium is one with monopolistic competition. The main conclusions are that the stock of human capital determines the rate of growth, that too little human capital is devoted to research in equilibrium, that integration into world markets will increase growth rates, and that having a large population is not sufficient to generate growth.

This paper presents a fully specified model of long-run growth in which knowledge is assumed to be an input in production that has increasing marginal productivity. It is essentially a competitive equilibrium model with endogenous technological change. In contrast to models based on diminishing returns, growth rates can be increasing over time, the effects of small disturbances can be amplified by the actions of private agents, and large countries may always grow faster than small countries. Long-run evidence is offered in support of the empirical relevance of these possibilities.

This paper describes two strands of work that converged under the heading of 'endogenous growth.' One strand, which is primarily empirical, asks whether there is a general tendency for poor countries to catch up with rich countries. The other strand, which is primarily theoretical, asks what modifications are necessary to construct a theory of aggregate growth that takes the economics of discovery, innovation, and technological change seriously. The paper argues that the second strand of work will ultimately have a more significant impact on our understanding of growth and our approach to aggregate theory.

In a world with two similar, developed economies, economic integration can cause a permanent increase in the worldwide rate of growth. Starting from a position of isolation, closer integration can be achieved by increasing trade in goods or by increasing flows of ideas. We consider two models with different specifications of the research and development sector that is the source of growth. Either form of integration can increase the long-run rate of growth if it encourages the worldwide exploitation of increasing returns to scale in the research and development sector.

This paper outlines a theoretical framework for thinking about the role of human capital in a model of endogenous growth. The framework pays particular attention to two questions: What are the theoretical differences between intangibles like education and experience on the one hand, and knowledge or science on the other? and How do knowledge and science actually affect production? One implication derived from this framework is that the initial level of a variable like literacy may be important for understanding subsequent growth. This emphasis on the level of an input contrasts with the usual emphasis from growth accounting on rates of change of inputs. The principal empirical finding is that literacy has no additional explanatory power in a cross-country regression of growth rates on investment and other variables, but consistent with the model, the initial level of literacy does help predict the subsequent rate of investment, and indirectly, the rate of growth.

A nation that lacks physical objects like factories and roads suffers from an object gap. A nation that lacks the knowledge used to create value in a modern economy suffers from an idea gap. Object gaps are emphasized by mainstream economists who make use of formal models and statistical hypothesis tests. Idea gaps are emphasized by dissident economists who make use of a diverse body of evidence and avoid formal models. Economists need to use the formal models from the first approach and the diverse evidence from the second to fully appreciate the importance of idea gaps in economic development.

The key step in understanding economic growth is to think carefully about ideas. This requires careful attention to the meaning of the words that we use and to the metaphors that we invoke when we construct mathematical models of growth. After addressing these issues, this paper describes two different ways in which ideas can contribute to economic development. The history of Mauritius shows how a poor economy can benefit by using ideas from industrial countries within its borders. The history of Taiwan (China) shows how a developing economy can be pushed forward into the ranks of those that produce ideas for sale on world markets.

To explain why trade restrictions sometimes speed up worldwide growth and sometimes slow it down, we exploit an analogy with the theory or consumer behavior. Substitution effects make demand curves slope down, but income effects can increase or decrease the slope, and can sometimes overwhelm the substitution effect. We decompose changes in the worldwide growth rate into two effects (integration and redundancy) that unambiguously slow down growth, and a third effect (allocation) that can either speed it up or slow it down. We study two types of trade restrictions to illustrate the use of this decomposition. The First is across the board restrictions on traded goods in an otherwise perfect market. The second is selective protection of knowledge-intensive goods in a world with imperfect intellectual property rights. In both examples, we show that for trade between similar regions such as Europe and North America, the first two effects dominate; starting from free trade, restrictions unambiguously reduce worldwide growth.

This article argues that to understand the behavior of productivity statistics, it is necessary to reexamine the basic assumptions underlying growth accounting. In particular, it offers theoretical and empirical support for the assertion that the elasticity of output with respect to an input like capital or labor might differ from the share of the input in total factor income. The theories offered in support of this possibility allow for spillovers of knowledge, specialization with monopolistic competition, and endogenous accumulation of labor-saving technological change. Evidence on the form of aggregate production is drawn from data for many countries and for long historical time periods. The specific interpretation of the productivity slowdown that is offered is that a low elasticity of output with respect to labor causes labor productivity growth rates to fall when labor growth speeds up.

In 1961, Nicholas Kaldor highlighted six "stylized" facts to summarize the patterns that economists had discovered in national income accounts and to shape the growth models being developed to explain them. Redoing this exercise today shows just how much progress we have made. In contrast to Kaldor's facts, which revolved around a single state variable, physical capital, our updated facts force consideration of four far more interesting variables: ideas, institutions, population, and human capital. Dynamic models have uncovered subtle interactions among these variables, generating important insights about such big questions as: Why has growth accelerated? Why are there gains from trade?

Everyday experience and a simple logical argument show that nonconvexities are essential for understanding growth. Compared to previous statements of this well known argument, the presentation here places more emphasis on the distinction between two of the fundamental attributes of any economic good: rivalry and excludability. It also emphasizes the difference between public goods and the technological advances that are fundamental to economic growth. Like public goods, technological advances are rionrival goods. Hence, they are inextricably linked to nonconvexities. But in contrast to public goods, which are nonexcludable, technological advances generate benefits that are at least partially excludable. Hence, innovations in the technology are for the most part privately provided. This means that nonconvexities are relevant for goods that trade in private markets. Consequently, an equilibrium with price-taking in all markets cannot be sustained. Concluding remarks describe some of the recent equilibrium growth models that do not rely on price-taking and highlight some of the implications of these models.

Wages for more- and less-educated workers have followed strikingly different paths in the U.S. and Canada. During the 1980's and 1990's, the ratio of earnings of university graduates to high school graduates increased sharply in the U.S. but fell slightly in Canada. Katz and Murphy (1992) found that for the U.S. a simple supply-demand model fit the pattern of variation in the premium over time. We find that the same model and parameter estimates explain the variation between the U.S. and Canada. In both instances, the relative demand for more-educated labor shifts out at the same, consistent rate. Both over time and between countries, the variation in rate of growth of relative wages can be explained by variation in the relative supply of more-educated workers. Many economists suspect that technological change is causing the steady increases in the relative demand for more-educated labor. If so, these data provide independent evidence on the spatial and temporal variation in the pattern of technological change. Whatever is causing this increased demand for skill, the evidence from Canada suggest that increases in educational attainment and skills can reduce the rate at which relative wages diverge.

This paper suggests that innovation policy in the United States has erred by subsidizing the private sector demand for scientists and engineers without asking whether the educational system provides the supply response necessary for these subsidies to work. It suggests that the existing institutional arrangements in higher education limit this supply response. To illustrate the path not taken, the paper considers specific programs that could increase the numbers of scientists and engineers available to the private sector.

From the beginning, growth theory has been faced with technically challenging questions about increasing returns and the way to capture ideas in a model of market exchange. Initially, reliance on perfect competition forced growth theory to narrow its scope. Recently, new tools for studying dynamic equilibria with nonconvexities, externalities, and imperfect competition have allowed growth theory to address broader questions like: Why have growth rates tended to increase over time? Why is it that flows of capital are not sufficient to equalize wages in different countries? How is it that trade policy, or aggregate research and development expenditure, or the extent of patent protection influences the rate of growth?

When they are used together, economic history and new growth theory give a more complete picture of technological change than either can give on its own. An empirical strategy for studying growth that does not use historical evidence is likely to degenerate into sterile model testing exercises. Historical analysis that uses the wrong kind of theory or no theory may not emphasize the lessons about technology that generalize. The complementarity between these fields is illustrated by an analysis of early industrialization. The key theoretical observation is that larger markets and larger stocks of resources create substantially bigger incentives for discovering new ways to use the resources. This simple insight helps explain why the techniques of mass production emerged in the United States during the first half of the 19th century. It also helps explain how a narrow advantage in the techniques of mass production for a small set of goods grew into broad position of industrial supremacy by the middle of the 20th century.

We study the hiring and retention of heterogeneous workers who learn over time. We show that the problem can be analyzed as an infinite-armed bandit with switching costs, and we apply results from Bergemann and Välimäki [Bergemann D, Välimäki J (2001) Stationary multi-choice bandit problems. J. Econom. Dynam. Control 25(10):1585–1594] to characterize the optimal hiring and retention policy. For problems with Gaussian data, we develop approximations that allow the efficient implementation of the optimal policy and the evaluation of its performance. Our numerical examples demonstrate that the value of active monitoring and screening of employees can be substantial.

Several well known integral stochastic orders (like the convex order, the supermodular order, etc.) can be defined in terms of the Hessian matrix of a class of functions. Here we consider a generic Hessian order, i.e., an integral stochastic order defined through a convex cone of Hessian matrices, and we prove that if two random vectors are ordered by the Hessian order, then their means are equal and the difference of their covariance matrices belongs to the dual of. Then we show that the same conditions are also sufficient for multinormal random vectors. We study several particular cases of this general result.

We identify a rich class of finite-horizon Markov decision problems (MDPs) for which the variance of the optimal total reward can be bounded by a simple linear function of its expected value. The class is characterized by three natural properties: reward nonnegativity and boundedness, existence of a do-nothing action, and optimal action monotonicity. These properties are commonly present and typically easy to check. Implications of the class properties and of the variance bound are illustrated by examples of MDPs from operations research, operations management, financial engineering, and combinatorial optimization.

This paper formulates an employer's hiring and retention decisions as an infinite-armed bandit problem and characterizes the structure of optimal hiring and retention policies. We develop approximations that allow us to explicitly calculate these policies and to evaluate their benefit. The solution involves a balance of two types of learning: the learning that reflects the improvement in performance of employees as they gain experience, and the Bayesian learning of employers as they infer properties of employees' abilities to inform the decision of whether to retain or replace employees. Numerical experiments with Monte Carlo simulation suggest that the gains to active screening and monitoring of employees can be substantial.

We consider the problem of selecting sequentially a unimodal subsequence from a sequence of independent identically distributed random variables, and we find that a person doing optimal sequential selection does so within a factor of the square root of two as well as a prophet who knows all of the random observations in advance of any selections. Our analysis applies in fact to selections of subsequences that have d+1 monotone blocks, and, by including the case d=0, our analysis also covers monotone subsequences.

We consider sequential selection of an alternating subsequence from a sequence of independent, identically distributed, continuous random variables, and we determine the exact asymptotic behavior of an optimal sequentially selected subsequence. Moreover, we find (in a sense we make precise) that a person who is constrained to make sequential selections does only about 12 percent worse than a person who can make selections with full knowledge of the random sequence.

Given a sequence of independent random variables with a common continuous distribution, we consider the online decision problem where one seeks to minimize the expected value of the time that is needed to complete the selection of a monotone increasing subsequence of a prespecified length n. This problem is dual to some online decision problems that have been considered earlier, and this dual problem has some notable advantages. In particular, the recursions and equations of optimality lead with relative ease to asymptotic formulas for mean and variance of the minimal selection time. 

In the secretary problem of Cayley (1875) and Moser (1956), n non-negative, independent, random variables with common distribution are sequentially presented to a decision maker who decides when to stop and collect the most recent realization. The goal is to maximize the expected value of the collected element. In the k-choice variant, the decision maker is allowed to make k≤n selections to maximize the expected total value of the selected elements. Assuming that the values are drawn from a known distribution with finite support, we prove that the best regret---the expected gap between the optimal online policy and its offline counterpart in which all n values are made visible at time 0---is uniformly bounded in the the number of candidates n and the budget k. Our proof is constructive: we develop an adaptive Budget-Ratio policy that achieves this performance. The policy selects or skips values depending on where the ratio of the residual budget to the remaining time stands relative to multiple thresholds that correspond to middle points of the distribution. We also prove that being adaptive is crucial: in general, the minimal regret among non-adaptive policies grows like the square root of n. The difference is the value of adaptiveness.

We prove a central limit theorem for a class of additive processes that arise naturally in the theory of finite horizon Markov decision problems. The main theorem generalizes a classic result of Dobrushin for temporally nonhomogeneous Markov chains, and the principal innovation is that here the summands are permitted to depend on both the current state and a bounded number of future states of the chain. We show through several examples that this added flexibility gives one a direct path to asymptotic normality of the optimal total reward of finite horizon Markov decision problems. The same examples also explain why such results are not easily obtained by alternative Markovian techniques such as enlargement of the state space.

We study the behavior of strategic customers in an open-routing service network with multiple stations. When a customer enters the network, she is free to choose the sequence of stations that she visits, with the objective of minimizing her expected total system time. We propose a two-station game with all customers present at the start of service and deterministic service times, and we find that strategic customers “herd,” that is, in equilibrium all customers choose the same route. For unobservable systems, we prove that the game is supermodular, and we then identify a broad class of learning rules—which includes both fictitious play and Cournot best response—that converges to herding in finite time. By combining different theoretical and numerical analyses, we find that the herding behavior is prevalent in many other congested open-routing service networks, including those with arrivals over time, those with stochastic service times, and those with more than two stations. We also find that the system under herding performs very close to the first-best outcome in terms of cumulative system time.

We study a dynamic and stochastic knapsack problem in which a decision maker is sequentially presented with n items with unitary rewards and independent weights that are drawn from a known continuous distribution F. The decision maker seeks to maximize the expected number of items that she includes in the knapsack while satisfying a capacity constraint, and while making terminal decisions as soon as each item weight is revealed. Under mild regularity conditions on the weight distribution F, we prove that the regret---the expected difference between the performance of the best sequential algorithm and that of a prophet who sees all of the weights before making any decision is, at most, logarithmic in n. Our proof is constructive. We devise a re-optimized heuristic that achieves this regret bound.

Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration-exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the 1930s, exploration–exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this monograph, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.

The multi-armed bandit problem for a gambler is to decide which arm of a K-slot machine to pull to maximize his total reward in a series of trials. Many real-world learning and optimization problems can be modeled in this way. Several strategies or algorithms have been proposed as a solution to this problem in the last two decades, but, to our knowledge, there has been no common evaluation of these algorithms.

A multi‐armed bandit is an experiment with the goal of accumulating rewards from a payoff distribution with unknown parameters that are to be learned sequentially. This article describes a heuristic for managing multi‐armed bandits called randomized probability matching, which randomly allocates observations to arms according the Bayesian posterior probability that each arm is optimal. Advances in Bayesian computation have made randomized probability matching easy to apply to virtually any payoff distribution. This flexibility frees the experimenter to work with payoff distributions that correspond to certain classical experimental designs that have the potential to outperform methods that are ‘optimal’ in simpler contexts. I summarize the relationships between randomized probability matching and several related heuristics that have been used in the reinforcement learning literature.

Optical trapping of dielectric particles by a single-beam gradient force trap was demonstrated for the first reported time. This confirms the concept of negative light pressure due to the gradient force. Trapping was observed over the entire range of particle size from 10 μm to ~25 nm in water. Use of the new trap extends the size range of macroscopic particles accessible to optical trapping and manipulation well into the Rayleigh size regime. Application of this trapping principle to atom trapping is considered.

Micron-sized particles have been accelerated and trapped in stable optical potential wells using only the force of radiation pressure from a continuous laser. It is hypothesized that similar accelerations and trapping are possible with atoms and molecules using laser light tuned to specific optical transitions. The implications for isotope separation and other applications of physical interest are discussed.

Optical trapping and manipulation of viruses and bacteria by laser radiation pressure were demonstrated with single-beam gradient traps. Individual tobacco mosaic viruses and dense oriented arrays of viruses were trapped in aqueous solution with no apparent damage using approximately 120 milliwatts of argon laser power. Trapping and manipulation of single live motile bacteria and Escherichia coli bacteria were also demonstrated in a high-resolution microscope at powers of a few milliwatts.

The stable levitation of small transparent glass spheres by the forces of radiation pressure has been demonstrated experimentally in air and vacuum down to pressures ∼1 Torr. A single vertically directed focused TEM00‐mode cw laser beam of ∼250 mW is sufficient to support stably a ∼20‐μ glass sphere. The restoring forces acting on a particle trapped in an optical potential well were probed optically by a second laser beam. At low pressures, effects arising from residual radiometric forces were seen. Possible applications are mentioned.

Use of lasers has revolutionized the study and applications of radiation pressure. Light forces have been achieved which strongly affect the dynamics of individual small particles. It is now possible to optically accelerate, slow, stably trap, and manipulate micrometer-sized dielectric particles and atoms. This leads to a diversity of new scientific and practical applications in fields where small particles play a role, such as light scattering, cloud physics, aerosol science, atomic physics, quantum optics, and high-resolution spectroscopy.

Reviews the history of optical trapping and manipulation of small-neutral particles, from the time of its origin in 1970 up to the present. As we shall see, the unique characteristics of this technique are having a major impact on the many subfields of physics, chemistry, and biology where small particles play a role.

We prove an optical radiation Earnshaw theorem: A small dielectric particle cannot be trapped by using only the scattering force of optical radiation pressure. A corollary is that the gradient or dipole force is necessary to any successful optical trap. We discuss the implications of the theorem for recent proposals for the optical trapping of neutral atoms.

Biological particles are successfully trapped in a single-beam gradient force trap using an infrared laser. The high numerical aperture lens objective in the trap is also used for simultaneous viewing. Several modes of trapping operation are presented.

The optical levitation technique has been extended to the study of light scattering from several basic types of nonspherical particles such as spheroids; spherical doublets, triplets, and quadruplets; and spheres within spheres. Individual particles were assembled in air using two independently maneuverable laser beams and then held at rest in the beam in fixed orientation. Observations were made of the light scattering in the near and far field with high resolution. Other applications of the technique are envisaged.

An in-line polarization rotator has been built into a single-mode birefringent fiber. The rotator utilizes periodic twists of the fiber’s principal axes, which were formed by rocking the preform as the fiber was drawn. The polarization conversion between the principal axes is wavelength dependent, with a bandwidth inversely proportional to the number of twist periods. The bandwidth of the present rotator was 4.8 nm for 100% conversion in a fiber length of 170 cm.

This paper illustrates with examples from the syntax of German how searching in corpora can help find theoretically relevant examples. Such examples are particularly interesting in that they exhibit a wide variation of potentially relevant parameters. The case studies highlight how linguistic terminology used to single out the relevant phenomenon can be reconstructed in terms of the empirical properties which are accessible directly or through annotations in a corpus.

This paper discusses some cultural differences in the organization of linguistic and sociological texts written by English and German speakers. Linearity, symmetry, hierarchy and continuity are examined in 52 texts as are the position of definitions and advance organizers and the integration of data. It is suggested that the differences between the English and German texts may be promoted by the education systems and by varying intellectual styles and attitudes to knowledge and content.

We present a log-linear model for the disambiguation of the analyses produced by a German broad-coverage LFG, focussing on the properties (or features) this model is based on. We compare this model to an initial model based only on a part of the properties provided to the final model and observe that the performance of a log-linear model for parse selection depends heavily on the types of properties that it is based on. In our case, the error reduction achieved with the log-linear model based on the extended set of properties is 51.0% and thus compares very favorably to the error reduction of 34.5% achieved with the initial model.

On the 17th of August 1308 Chiara of Montefalco died in the small Umbrian monastery of which she had been the abbess. Her fellow nuns did not take any steps to preserve her body. Nonetheless, for five days it remained uncorrupted and redolent of the odor of sanctity, despite the blazing summer heat. At that point— not wanting to tempt fate further—the community decided to embalm the precious relic. In the words of Sister Francesca of Montefalco, testifying some years later at Chiara's unsuccessful canonization procedure, “They agreed that [her] body should be preserved on account of her holiness and because God took such pleasure in her body and her heart.” They sent to the town apothecary for “balsam and myrrh and other preservatives,” as the apothecary himself testified, and they proceeded to the next step in contemporary embalming practice, which was evisceration.

Although Max Weber believed that rational capitalism developed initially and primarily under Protestantism, it was born and developed extensively in pre-Reformation Italy. Protestant Europe borrowed its rational business techniques from Italy, then later made its own contribution. Thus, capitalist rationality advanced under both Roman Catholicism and Protestantism, and the religious factor had little effect on its early development.

The Infamous Black Death of 1348 signaled the reappearance of bubonic plague in Europe after long centuries of absence. Contemporary accounts vividly describe the shock and horror of universal and indiscriminate mortality. From Tournai, Gilles Li Muisis observed that “no one was secure, whether rich, in moderate circumstances or poor, but everyone from day to day waited on the will of the Lord.” In any given area, between one third to half of the population would die. Worse still, the Black Death was only the beginning of a worldwide pandemic, or cyclical series of epidemics, recurring at intervals of two to twenty years throughout Europe until well into the seventeenth century.

Thirty-five years ago Robert Lopez, by his own description, “narrowly escaped lynching” at the hands of non-economic historians for proposing that economic depression was a fundamental cause of the cultural outpouring of the Renaissance (Lopez, 1953). Several years later, Lopez took heart that, despite “their occasional retard,” cultural historians were coming round to his view (Lopez and Miskimin, 408-09). Today, the situation is nearly reversed. A growing number of economic historians no longer subscribe to the depression thesis while most non-economic historians do. I will not speculate about whose “retardation” is to blame, but would like to take stock of some issues raised by the depression debate—a debate that transcends economic issues and raises important questions about definitions, periodization, and the cultural implications of economic conditions.

Social network sites (SNSs) are increasingly attracting the attention of academic and industry researchers intrigued by their affordances and reach. This special theme section of the Journal of Computer‐Mediated Communication brings together scholarship on these emergent phenomena. In this introductory article, we describe features of SNSs and propose a comprehensive definition. We then present one perspective on the history of such sites, discussing key changes and developments. After briefly summarizing existing scholarship concerning SNSs, we discuss the articles in this special section and conclude with considerations for future research.

We designed an experiment to study trust and reciprocity in an investment setting. This design controls for alternative explanations of behavior including repeat game reputation effects, contractual precommitments, and punishment threats. Observed decisions suggest that reciprocity exists as a basic element of human behavior and that this is accounted for in the trust extended to an anonymous counterpart. A second treatment, social history, identifies conditions which strengthen the relationship between trust and reciprocity.

Analysis of the Greenland ice core covering the period from 3000 to 500 years ago—the Greek, Roman, Medieval and Renaissance times—shows that lead is present at concentrations four times as great as natural values from about 2500 to 1700 years ago (500 B.C. to 300 A.D.). These results show that Greek and Roman lead and silver mining and smelting activities polluted the middle troposphere of the Northern Hemisphere on a hemispheric scale two millennia ago, long before the Industrial Revolution. Cumulative lead fallout to the Greenland Ice Sheet during these eight centuries was as high as 15 percent of that caused by the massive use of lead alkyl additives in gasoline since the 1930s. Pronounced lead pollution is also observed during Medieval and Renaissance times.

In the last years many Roman and pre-Roman hydraulic tunnels in central Italy have been explored; here they are described and discussed. These engineering works were built according to complex projects and by using sophisticated techniques. These findings are compared with similar findings in Greece, in the tunnel of Samos, and in the outlet of Kopais, the latter which could be attributed to the Mycenaean civilisation, namely to the 12th century BC. 

Shallow and deepwater seabed exploration in the Black Sea conducted in 2000 and 2003 as part of a long term project developed by Robert Ballard’s Institute for Exploration (IFE) identified a number of shipwrecks and environmental features while testing survey methodology and equipment for deepwater archaeological applications.1 The underwater efforts complemented traditional terrestrial surveys in the Sinope area and contributed to a robust debate on the flooding of the Black Sea. This contribution outlines the maritime survey and methodology used to locate four ships (4th to 6th c.) in 2000 and interprets data recovered from those sites in 2000 and 2003.2

New 14C dates from Oaxaca, Mexico, document changes in religious ritual that accompanied the evolution of society from hunting and gathering to the archaic state. Before 4000 B.P. in conventional radiocarbon years, a nomadic egalitarian lifeway selected for unscheduled (ad hoc) ritual from which no one was excluded. With the establishment of permanent villages (4000–3000 B.P.), certain rituals were scheduled by solar or astral events and restricted to initiates/social achievers. After state formation (2050 B.P.), many important rituals were performed only by trained full-time priests using religious calendars and occupying temples built by corvée labor. Only 1,300–1,400 years seem to have elapsed between the oldest known ritual building and the first standardized state temple.

In response to Smith’s critique, we situate our work within the scientific method. We review how our hypotheses arose, how and by whom they have been tested, and with what results. Like Smith, we focus on whether Maya civic centers include emphatic expression of a north-south axis, and on the inference of a partially cosmological basis of Maya axial orders. We indicate sources of our inferences and review explicitly the criteria that Smith faults us for having omitted. We reaffirm that these ideas merit further testing, and welcome renewed and expanded collaboration as to the best methods for doing so. In the spirit of collaboration, we offer specific programmatic suggestions for how this inquiry might proceed.

The noble colors blue and purple were highly regarded in the past because of their rarity. Thus ancient civilizations invented blue and purple pigments, such as, Egyptian Blue (see amulet shown), Chinese Blue, and Chinese Purple, all of which contain alkaline‐earth metals and copper. It is shown how their synthesis may have been developed in ancient times: an understanding of stoichiometry, the control of reaction temperature, and the hypothesis of knowledge transfer are essential.

While acknowledging the need for “sustainability,” this paper summarizes the problems that have been encountered in our understanding and use of this concept. It explores the efforts of others to define the concept within the context of specific disciplinary areas and sets forth a proposal for a basic understanding of the term “environmental sustainability” as an expansion of our common perception of the nature of human activity so as to more clearly connect it with the ecological concept of interdependence and to serve as a goal for environmental managers. 

The authors review three theoretical approaches to strategic corporate social responsibility (CSR), which can be defined as voluntary CSR actions that enhance a firm’s competitiveness and reputation. The end result of such activities should be an improvement in financial and economic performance. Based on an overview of recent empirical evidence, the authors conclude that economic theories of strategic CSR have the greatest potential for advancing this field of inquiry, although theories of strategic leadership should also be incorporated into this perspective. In the remainder of the article, they provide focused summaries of the articles presented in this special issue and outline an agenda for future research on strategic CSR and environmental sustainability.

Efforts to ‘operationalize’ a concept of sustainability into appraisal methods for practical decisionmaking have been few and generally unpersuasive. In this paper it is argued that this need not be the case if a set of environmentally compensating, or ‘shadow’, projects within an overall portfolio are used to ensure a sustainability objective of setting a constraint on the depletion and degradation of the stock of natural capital. This can be achieved through both a ‘weak’ and a ‘strong’ sustainability criterion. In both cases the resulting optimum differs from the efficient optimum of the conventional cost-benefit criterion, but the basic cost-benefit model remains intact.

Sustainable supply management research generally focuses on environmental practices. We show through an analysis of the food industry that sustainability requires an expanded view to encompass both environmental and social elements. We interviewed and surveyed food and beverage producers in the U.S. Pacific Northwest to both validate expanded sustainability elements in the industry and assess subsequent performance outcomes. A path analysis reveals that food industry managers perceive both direct and mediated impacts of sustainability programs on performance. Specifically, the results indicate that sustainability program effects are limited to the impact of conservation and land management environmental practices on overall environmental performance and human resources practices on quality performance. However, environmental performance improvements lead to improved quality performance, which in turn improves cost performance. The results highlight the complexity of sustainability impacts on performance and suggest that performance benefits from sustainability programs may be difficult to recognize.

To provide practitioners with useful information about how to promote proenvironmental behavior (PEB), a meta-analysis was performed on 87 published reports containing 253 experimental treatments that measured an observed, not self-reported, behavioral outcome. Most studies combined multiple treatments, and this confounding precluded definitive conclusions about which individual treatments are most effective. Treatments that included cognitive dissonance, goal setting, social modeling, and prompts provided the overall largest effect sizes. Further analyses indicated that different treatments have been more effective for certain behaviors. Although average effect sizes are based on small numbers of studies, effective combinations of treatments and behaviors are making it easy to recycle, setting goals for conserving gasoline, and modeling home energy conservation. The results also reveal several gaps in the literature that should guide further research, including both treatments and PEB that have not been tested.

For the past three centuries, the effects of humans on the global environment have escalated. Because of these anthro-pogenic emissions of carbon dioxide, global climate may depart significantly from natural behaviour for many millennia to come. It seems appropriate to assign the term ‘Anthropocene’ to the present, in many ways human-dominated, geological epoch, supplementing the Holocene—the warm period of the past 10–12 millennia.

One reason for the poor immunogenicity of many tumors may be that they cannot provide signals for CD28-mediated costimulation necessary to fully activate T cells. It has recently become apparent that CTLA-4, a second counterreceptor for the B7 family of costimulatory molecules, is a negative regulator of T cell activation. Here, in vivo administration of antibodies to CTLA-4 resulted in the rejection of tumors, including preestablished tumors. Furthermore, this rejection resulted in immunity to a secondary exposure to tumor cells. These results suggest that blockade of the inhibitory effects of CTLA-4 can allow for, and potentiate, effective immune responses against tumor cells.

Immune checkpoint therapy, which targets regulatory pathways in T cells to enhance antitumor immune responses, has led to important clinical advances and provided a new weapon against cancer. This therapy has elicited durable clinical responses and, in a fraction of patients, long-term remissions where patients exhibit no clinical signs of cancer for many years. The way forward for this class of novel agents lies in our ability to understand human immune responses in the tumor microenvironment. This will provide valuable information regarding the dynamic nature of the immune response and regulation of additional pathways that will need to be targeted through combination therapies to provide survival benefit for greater numbers of patients.

A variety of tumors are potentially immunogenic but do not stimulate an effective anti-tumor immune response in vivo. Tumors may be capable of delivering antigen-specific signals to T cells, but may not deliver the costimulatory signals necessary for full activation of T cells. Expression of the costimulatory ligand B7 on melanoma cells was found to induce the rejection of a murine melanoma in vivo. This rejection was mediated by CD8+ T cells; CD4+ T cells were not required. These results suggest that B7 expression renders tumor cells capable of effective antigen presentation, leading to their eradication in vivo.

PD-1, a 55 kDa transmembrane protein containing an immunoreceptor tyrosine-based inhibitory motif, is induced in lymphocytes and monocytic cells following activation. Aged C57BL/6(B6)-PD-1−/− congenic mice spontaneously developed characteristic lupus-like proliferative arthritis and glomerulonephritis with predominant IgG3 deposition, which were markedly accelerated by introduction of a Fas mutation (lpr). Introduction of a PD-1 null mutation into the 2C-TCR (anti-H-2Ld) transgenic mice of the H-2b/d background resulted in the chronic and systemic graft-versus-host-like disease. Furthermore, CD8+2C-TCR+PD-1−/− T cells exhibited markedly augmented proliferation in vitro in response to H-2d allogenic cells. Collectively, it is suggested that PD-1 is involved in the maintenance of peripheral self-tolerance by serving as a negative regulator of immune responses.

PD-1 is a receptor of the Ig superfamily that negatively regulates T cell antigen receptor signaling by interacting with the specific ligands (PD-L) and is suggested to play a role in the maintenance of self-tolerance. In the present study, we examined possible roles of the PD-1/PD-L system in tumor immunity. Transgenic expression of PD-L1, one of the PD-L, in P815 tumor cells rendered them less susceptible to the specific T cell antigen receptor-mediated lysis by cytotoxic T cells in vitro, and markedly enhanced their tumorigenesis and invasiveness in vivo in the syngeneic hosts as compared with the parental tumor cells that lacked endogenous PD-L. Both effects could be reversed by anti-PD-L1 Ab. Survey of murine tumor lines revealed that all of the myeloma cell lines examined naturally expressed PD-L1. Growth of the myeloma cells in normal syngeneic mice was inhibited significantly albeit transiently by the administration of anti-PD-L1 Ab in vivo and was suppressed completely in the syngeneic PD-1-deficient mice. These results suggest that the expression of PD-L1 can serve as a potent mechanism for potentially immunogenic tumors to escape from host immune responses and that blockade of interaction between PD-1 and PD-L may provide a promising strategy for specific tumor immunotherapy.

The circadian clock is a widespread cellular mechanism that underlies diverse rhythmic functions in organisms from bacteria and fungi, to plants and animals. Intense genetic analysis during recent years has uncovered many of the components and molecular mechanisms comprising these clocks. Although autoregulatory genetic networks are a consistent feature in the design of all clocks, the weight of evidence favours their independent evolutionary origins in different kingdoms.

Eclosion, or emergence of adult flies from the pupa, and locomotor activity of adults occur rhythmically in Drosophila melanogaster, with a circadian period of about 24 hours. Here, a clock mutation, timeless (tim), is described that produces arrhythmia for both behaviors. The effects of tim on behavioral rhythms are likely to involve products of the X chromosome-linked clock gene period (per), because tim alters circadian oscillations of per RNA. Genetic mapping places tim on the left arm of the second chromosome between dumpy (dp) and decapentaplegic (dpp).

Two genes, period (per) and timeless (tim), are required for production of circadian rhythms in Drosophila. The proteins encoded by these genes (PER and TIM) physically interact, and the timing of their association and nuclear localization is believed to promote cycles of per and tim transcription through an autoregulatory feedback loop. Here it is shown that TIM protein may also couple this molecular pacemaker to the environment, because TIM is rapidly degraded after exposure to light. TIM accumulated rhythmically in nuclei of eyes and in pacemaker cells of the brain. The phase of these rhythms was differentially advanced or delayed by light pulses delivered at different times of day, corresponding with phase shifts induced in the behavioral rhythms.

Mutations in the period (per) gene of Drosophila melanogaster affect both circadian and ultradian rhythms. Levels of per gene product undergo circadian oscillation, and it is now shown that there is an underlying oscillation in the level of per RNA. The observations indicate that the cycling of per-encoded protein could result from per RNA cycling, and that there is a feedback loop through which the activity of per-encoded protein causes cycling of its own RNA.

The mechanisms by which circadian pacemaker systems transmit timing information to control behavior are largely unknown. Here, we define two critical features of that mechanism in Drosophila. We first describe animals mutant for the pdf neuropeptide gene, which is expressed by most of the candidate pacemakers (LNv neurons). Next, we describe animals in which pdf neurons were selectively ablated. Both sets of animals produced similar behavioral phenotypes. Both sets entrained to light, but both were largely arrhythmic under constant conditions. A minority of each pdf variant exhibited weak to moderate free-running rhythmicity. These results confirm the assignment of LNv neurons as the principal circadian pacemakers controlling daily locomotion in Drosophila. They also implicate PDF as the principal circadian transmitter.

Transgenic Drosophila that expressed either luciferase or green fluorescent protein driven from the promoter of the clock geneperiod were used to monitor the circadian clock in explanted head, thorax, and abdominal tissues. The tissues (including sensory bristles in the leg and wing) showed rhythmic bioluminescence, and the rhythms could be reset by light. The photoreceptive properties of the explanted tissues indicate that unidentified photoreceptors are likely to contribute to photic signal transduction to the clock. These results show that autonomous circadian oscillators are present throughout the body, and they suggest that individual cells in Drosophilaare capable of supporting their own independent clocks.

The concept that all living animals can be arranged along a continuous "phylogenetic scale" with man at the top is inconsistent with contemporary views of animal evolution. Nevertheless, this arbitrary hierarchy continues to influence researchers in the field of animal behavior who seek to make inferences about the evolutionary development of a particular type of behavior. Comparative psychologists have failed to distinguish between data obtained from living representatives of a common evolutionary lineage and data from animals which represent divergent lineages. Only the former can provide a foundation for inferences about the phylogenetic development of behavior patterns. The latter can provide information only about general mechanisms of adaptation and survival, which are not necessarily relevant to any specific evolutionary lineage. The widespread failure of comparative psychologists to take into account the zoological model of animal evolution when selecting animals for study and when interpreting behavioral similarities and differences has greatly hampered the development of generalizations with any predictive value.

Cryo-electron microscopy of vitrified specimens was just emerging as a practical method when Richard Henderson proposed that we should teach an EMBO course on the new technique. The request seemed to come too early because at that moment the method looked more like a laboratory game than a useful tool. However, during the months which ellapsed before the start of the course, several of the major difficulties associated with electron microscopy of vitrified specimens found surprisingly elegant solutions or simply became non-existent. The course could therefore take place under favourable circumstances in the summer of 1983. It was repeated the following years and cryo-electron microscopy spread rapidly. Since that time, water, which was once the arch enemy of all electronmicroscopists, became what it always was in nature – an integral part of biological matter and a beautiful substance.

Thin vitrified layers of unfixed, unstained and unsupported virus suspensions can be prepared for observation by cryo-electron microscopy in easily controlled conditions. The viral particles appear free from the kind of damage caused by dehydration, freezing or adsorption to a support that is encountered in preparing biological samples for conventional electron microscopy. Cryo-electron microscopy of vitrified specimens offers possibilities for high resolution observations that compare favourably with any other electron microscopical method.

What are the components that control the assembly of subcellular organelles in eukaryotic cells? Although membranes can clearly be distorted by cytosolic factors, very little is known about the intrinsic mechanisms that control the biogenesis, shape, and organization of organellar membranes. Here, we found that the unconventional phospholipid lysobisphosphatidic acid (LBPA) could induce the formation of multivesicular liposomes that resembled the multivesicular endosomes that exist where this lipid is found in vivo. This process depended on the same pH gradient that exists across endosome membranes in vivo and was selectively controlled by Alix. In turn, Alix regulated the organization of LBPA-containing endosomes in vivo.

Since the beginning of the 1980s, cryo‐electron microscopy of a thin film of vitrified aqueous suspension has made it possible to observe biological particles in their native state, in the absence of the usual artefacts of dehydration and staining. Combined with 3‐d reconstruction, it has become an important tool for structural molecular biology. Larger objects such as cells and tissues cannot generally be squeezed in a thin enough film. Cryo‐electron microscopy of vitreous sections (CEMOVIS) provides then a solution. It requires vitrification of a sizable piece of biological material and cutting it into ultrathin sections, which are observed in the vitrified state. Each of these operations raises serious difficulties that have now been overcome. In general, the native state seen with CEMOVIS is very different from what has been seen before and it is seen in more detail. CEMOVIS will give its full potential when combined with computerized electron tomography for 3‐d reconstruction.

This essay presents a theoretical description of the superradiance phenomenon, in which both the quantal and the classical aspects are discussed. Starting from the simple two-level atom-small sample Dicke model, we successively introduce various complications inherent to a realistic superradiance experiment: effects of Van der Waals interaction between the atoms, propagation and diffraction of the electromagnetic field in the sample and finally the effects related to atomic level degeneracy or near degeneracy. We recall how to calculate the field radiated by a superradiant system in a single experiment and how to determine, for a series of identically prepared superradiant samples, the large shot to shot fluctuations of the emitted light properties. The presentation tries to unify various points of view and formalisms developed in previous works and to introduce simply and progressively the basic physical concepts relevant to the superradiance phenomenon.

Pairs of atoms have been prepared in an entangled state of the Einstein-Podolsky-Rosen (EPR) type. They were produced by the exchange of a single photon between the atoms in a high Q cavity. The atoms, entangled in a superposition involving two different circular Rydberg states, were separated by a distance of the order of 1 cm. At variance with most previous EPR experiments, this one involves massive particles. It can be generalized to three or more atoms and opens the way to new tests of nonlocality in mesoscopic quantum systems.